package kafka.producer

import org.apache.kafka.clients.producer.ProducerConfig
import org.apache.hadoop.shaded.org.eclipse.jetty.util.thread.ExecutionStrategy.Producer

//  https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html

object KafkaProducerConfig {

  sealed trait CompressionType
  case object none extends CompressionType
  case object gzip extends CompressionType
  case object snappy extends CompressionType
  case object lz4 extends CompressionType
  case object zstd extends CompressionType

  sealed trait PartitionStrategy
  case object DefaultPartitioner extends PartitionStrategy
  case object RoundRobin extends PartitionStrategy

  def createProducerConfig(
      bootstrapServers: String,
      bufferMemory: Option[Long] = Some(33554432L),
      compressionType: Option[CompressionType] = Some(none),
      partitionStrategy: Option[PartitionStrategy] = None,
      batchSize: Option[Long] = Some(16384L),
      enableIdempotence: Option[Boolean] = None,
      acks: Option[String] = None
  ): Map[String, Object] = {
    val baseConfig = Map[String, Object](
      ProducerConfig.BOOTSTRAP_SERVERS_CONFIG -> bootstrapServers,
      ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG -> "org.apache.kafka.common.serialization.StringSerializer",
      ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG -> "org.apache.kafka.common.serialization.StringSerializer"
    ) ++ bufferMemory.map(ProducerConfig.BUFFER_MEMORY_CONFIG -> _.toString) ++
      compressionType.map(
        ProducerConfig.COMPRESSION_TYPE_CONFIG -> compressionTypeToString(_)
      ) ++
      batchSize.map(ProducerConfig.BATCH_SIZE_CONFIG -> _.toString) ++
      enableIdempotence.map(
        ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG -> _.toString
      ) ++
      acks.map(ProducerConfig.ACKS_CONFIG -> _)

    val partitionerConfig = partitionStrategy match {
      case Some(RoundRobin) =>
        Map(
          ProducerConfig.PARTITIONER_CLASS_CONFIG -> "org.apache.kafka.clients.producer.RoundRobinPartitioner"
        )
      case Some(DefaultPartitioner) | None =>
        Map.empty[String, Object]
      /* Extend with additional strategies as needed */
      case _ =>
        Map.empty[String, Object]
    }

    baseConfig ++ partitionerConfig

  }

  private def compressionTypeToString(
      compressionType: CompressionType
  ): String = compressionType match {
    case `none`   => "none"
    case `gzip`   => "gzip"
    case `snappy` => "snappy"
    case `lz4`    => "lz4"
    case `zstd`   => "zstd"
  }

}

/*
buffer.memory (default 33554432)

The total bytes of memory the producer can use to buffer records waiting to be sent
to the server.
If records are sent faster than they can be delivered to the server the
producer will block for max.block.ms after which it will throw an exception.
This setting should correspond roughly to the total memory the producer will use,
but is not a hard bound since not all memory the producer uses is used for buffering.
Some additional memory will be used for compression
(if compression is enabled) as well as for maintaining
in-flight requests.

compression.type
The compression type for all data generated by the producer.
The default is none (i.e. no compression).
Valid values are none, gzip, snappy, lz4, or zstd.
Compression is of full batches of data, so the efficacy of batching will also impact the compression
ratio (more batching means better compression).


batch.size

The producer will attempt to batch records together into fewer requests
whenever multiple records are being sent to the same partition.
This helps performance on both the client and the server.
This configuration controls the default batch size in bytes.

No attempt will be made to batch records larger than this size.

Requests sent to brokers will contain multiple batches,
one for each partition with data available to be sent.

A small batch size will make batching less common and
may reduce throughput (a batch size of zero will disable batching entirely).
A very large batch size may use memory a bit more wastefully as we will always
allocate a buffer of the specified batch size in anticipation of additional records.

Note: This setting gives the upper bound of the batch size to be sent.
If we have fewer than this many bytes accumulated for this partition,
we will ‘linger’ for the linger.ms time waiting for more records to show up.
This linger.ms setting defaults to 0, which means we’ll immediately
send out a record even the accumulated batch size is under this batch.size setting.

Type:	int
Default:	16384
Valid Values:	[0,…]

case class BatchSize(sizeBytes: Long)

linger.ms
The producer groups together any records that arrive in between request transmissions
into a single batched request.
Normally this occurs only under load when records arrive faster than they can be sent out.
However in some circumstances the client may want to reduce the number of requests
even under moderate load. This setting accomplishes this by adding a small amount of artificial delay
— that is, rather than immediately sending out a record, the producer will wait
for up to the given delay to allow other records to be sent so that the sends can be
batched together.

Analogous to Nagle’s algorithm in TCP


partitioner.class

Determines which partition to send a record to when records are produced. Available options are:

Default Strategy:
  1. Key Present          -> use the Hash of the Key.
  2. No Partition ,No Key -> Choose the Sticky Partition that changes every batch.size bytes to that partition.


RoundRobinPartitioner Strategy:
- Cycles through Partitions one by one to evenly distribute messages.

org.apache.kafka.clients.producer.Partitioner interface can be implemented for Custom Partitioners.




enable.idempotent=true
- Used for Only-Once Semantics
- The default value isfalse, meaning a producer may write duplicate copies of a message to the stream. To turn idempotence on, use the below command.

acks
Default  : all

acks=0   : Setting acks to 0 means the producer will not get any acknowledgment from the server at all. This means that the record will be immediately added to the socket buffer and considered sent.
acks=1   : This means that as long as the producer receives an acknowledgment from the leader broker, it would consider it as a successful commit.
acks=all : This means the producer will have to wait for acknowledgments from all the in-sync replicas of that topic before considering a successful commit. It gives the strongest available message durability.

 */
